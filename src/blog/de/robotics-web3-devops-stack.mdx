---
title: Robotics Web3 DevOps Stack
date: 2024-06-12
published: true
locale: 'de'
tags: ['DevOps', 'Web3']
cover_image: /blog/images/web3-devops-stack/web3-devops-stack-Cover.png
description: "Die DevOps-Bewegung hat die IT-Welt revolutioniert und unser Verständnis von effektiver, kontinuierlicher Systemverbesserung neu geformt. Das Gebiet der Robotik beginnt gerade erst, diesen Ansatz zu erkunden. In diesem Artikel skizzieren wir die potenzielle Zukunft von DevOps für Robotik unter Verwendung von Web3-Technologien. Wir heben auch einige Projekte hervor, die die Grundlage für diese Zukunft legen könnten."
abstract: "Die DevOps-Bewegung hat die IT-Welt revolutioniert und unser Verständnis von effektiver, kontinuierlicher Systemverbesserung neu geformt. Das Gebiet der Robotik beginnt gerade erst, diesen Ansatz zu erkunden. In diesem Artikel skizzieren wir die potenzielle Zukunft von DevOps für Robotik unter Verwendung von Web3-Technologien. Wir heben auch einige Projekte hervor, die die Grundlage für diese Zukunft legen könnten."
---

## Ursprünge von Entwicklung & Betrieb

Roboter, als Geräte, die sich in der äußeren Welt bewegen und diese wahrnehmen, sind ein spezieller Fall von Computern. Wie Ingenieure oft sagen, ist ein Flugzeug ein fliegender Computer, was die Bedeutung des an Bord befindlichen Rechengeräts für die Aufrechterhaltung der Lebensfähigkeit dieses cyber-physischen Systems betont. Daher können wir betrachten, wie sich die Welt der Informationssysteme im 21. Jahrhundert entwickelt hat und bestimmte Trends identifizieren, die in der Robotik in naher Zukunft inhärent sein werden.

Software und die für ihre Entwicklung verwendeten Praktiken haben die Arbeitsansätze sowohl in der Ingenieurwissenschaft als auch in jeder anderen Tätigkeit erheblich verändert. Vor allem betrafen die Veränderungen den sogenannten Wasserfallansatz, bei dem jedes Projekt als eine Reihe von langen abwechselnden Phasen angesehen wurde: Anforderungen - Entwicklung - Verifikation - Validierung - Betrieb. Im Laufe der Zeit stellte sich heraus, dass die Unterteilung in Phasen für Manager praktisch ist, aber nicht ganz dem entspricht, wie es im Leben passiert, und die schnelle Anpassung von Produkten in den Bedingungen einer sich ständig verändernden äußeren Welt erschwert. Agile Methoden, die eine synchrone Arbeit am Projekt mit kleinen Planungsintervallen implizieren, ersetzten das Wasserfallmodell der Aktivität (und des Managements). Anstelle großer Etappen mit Meilensteinen gab es kleine Sprints mit Inkrementen - kleine, aber häufige Änderungen in den entwickelten Systemen mit einer schnellen Veröffentlichung. Ein solches Arbeitstempo ist unerreichbar, wenn die Veröffentlichung von komplexen Verwaltungsverfahren mit manuellen Überprüfungen begleitet wird. So entstand die Bewegung Entwicklung & Betrieb, um Praktiken zur Übertragung von Wert von Entwicklern auf Verbraucher ohne Verluste umzusetzen und einen neuen Beruf eines DevOps-Spezialisten zu schaffen.Ops-Ingenieur, der den Prozess der schnellen Inbetriebnahme des Systems automatisiert.

![DevOps Ursprünge](/blog/images/web3-devops-stack/devops-evolution.png)

Diese Praktiken umfassen automatisierte Tests, kontinuierliche Integration, Bereitstellung, Bereitstellung und Softwareüberwachung. Automatisierte Tests überprüfen auf Fehler, während die kontinuierliche Integration Codeänderungen häufig integriert, um Probleme frühzeitig zu erkennen. Bereitstellung und Bereitstellung beinhalten das Bereitstellen der Software für Benutzer und deren Installation in ihrer Umgebung.

Ursprünglich gewannen diese Praktiken an Dynamik im Bereich großer Webservice, die Millionen von Benutzern auf der ganzen Welt bedienen. In einem solchen Kontext sind diese Praktiken entscheidend, um die hohe Leistung und Zuverlässigkeit aufrechtzuerhalten, die von solchen Diensten erwartet werden. Mit der Zeit und der zunehmenden Zugänglichkeit von Automatisierungstools (teilweise aufgrund von Open-Source-Beiträgen) begann DevOps jedoch, in traditionellere Arten von Ingenieurtätigkeiten einzudringen. Daher haben diese Praktiken ihren ursprünglichen Bereich überschritten und werden nun in verschiedenen Sektoren eingesetzt, um die Effizienz, Zuverlässigkeit und Qualität von Softwareprodukten zu verbessern.

## Hauptmerkmale von Robotern im Kontext von DevOps

Roboter bewegen sich im Raum und nehmen die externe Umgebung wahr, im Gegensatz zu stationären Servern und Rechenzentren. Immer mehr solcher Geräte tauchen auf. Wir sehen, wie ihre Rechenleistung im Laufe der Zeit wächst - viele technische Lösungen wären ohne einen leistungsstarken Onboard-Prozessor schlichtweg unmöglich.

Mit dem Anstieg des Edge Computing, das sich auf das Computing auf den mobilen Geräten selbst bezieht, wird die Verwendung von DevOps immer beliebter. Dieser Ansatz ermöglicht eine schnelle Bereitstellung neu entwickelter Funktionen für Verbraucher. Roboter können kontinuierlich mit dem Netzwerk verbunden sein und ihre Software mithilfe von Docker und anderen in der Praxis bewährten Tools schnell aktualisieren.

Allerdings schaffen zusätzliche Fähigkeiten zusätzliche Risiken - indem wir Computern die Möglichkeit geben, intensiv mit der externen Umgebung und dem Internet gleichzeitig zu interagieren, schaffen wir Voraussetzungen für potenzielle Schäden, die diese Geräte anrichten können, wenn sie in die Hände von Übeltätern geraten. Intelligente Geräte, die allmählich unsere Häuser füllen, könnten plötzlich verrückt spielen! Das Thema Sicherheit ist in einer sich ständig verändernden Welt von größter Bedeutung, da sich all diese Geräte in unmittelbarer Nähe zu unseren Körpern befinden. Fehler können zu teuer werden.

Was macht Robotik besonders und unterscheidet sie von anderen Computern? Diese besonderen Merkmale umfassen:

1. Eine Vielzahl von Hardwareplattformen (im Vergleich zu einer relativ geringen Anzahl von zentralen Prozessorarchitekturtypen)
2. Arbeiten in unvorhersehbaren Umgebungen, was das Vorhandensein von Notfallsituationen impliziert - beispielsweise die Nichtverfügbarkeit über Kommunikationskanäle
3. Begrenzte Ressourcen von On-Board-Computing-Geräten - schwacher Prozessor, relativ geringe Speichermenge und niedrige Netzwerkbandbreite4. Erhöhte Sicherheitsanforderungen
5. Eine große Anzahl von Geräten, die Updates und neue Funktionen erfordern

Unter Berücksichtigung dieser Einschränkungen werden wir versuchen, eine konzeptionelle Grundlage für DevOps für Roboter auf der Grundlage von Web3-Technologien zu formulieren, zumindest für das Schreiben von Code, das Erstellen und Bereitstellen. Wir werden versuchen, die interessantesten Web3-Projekte vorzustellen, die in der Lage sind, den Betrieb eines dezentralen DevOps-Förderbands sicherzustellen, und zeigen, wie Robonomics ein integraler Bestandteil davon werden kann in der Endphase des Lebenszyklus - bei der Implementierung von Programmen in der äußeren Welt.

## Web3 und Blockchain

Wenn wir über Web3 sprechen, meinen wir in erster Linie die Blockchain-Technologie. Dies sind natürlich eng verwandte Konzepte, aber nicht genau identisch. Oft wird unter diesem Konzept ein ganzes Set von Technologien verstanden - elliptische Kurvenkryptographie, P2P-Netzwerke, Konsensalgorithmus. Die ersten beiden Punkte sind nichts Einzigartiges und werden weit verbreitet in der IT-Branche eingesetzt, während der letztere der Blockchain wirklich eine besondere Eigenschaft verleiht und diese Lösung zu einem so leistungsstarken Werkzeug macht. Aber brauchen wir für alle Lebenssituationen (und Teile des DevOps-Förderbands) Konsens? Lassen Sie uns versuchen, das herauszufinden.

Konsens hat in verschiedenen Kontexten unterschiedliche Bedeutungen, aber seine Essenz ist immer gleich - Systeme, Akteure oder autonome Agenten, die ein dezentrales Netzwerk bilden, einigen sich darauf, welches Protokoll sie bei der Arbeit und dem Zustand gemeinsamer Daten verwenden. Wenn die Parteien keine Einigung erzielen, führt dies zum sogenannten Fork. Historisch gesehen bedeutete ein Fork eine Aufspaltung der Blockchain, die zur Teilung der Netzwerkknoten in zwei Teile führte - einige Knoten blieben "loyal" zur alten Version des Protokolls, während der Rest zur neuen Version wechselte. Dieses Ereignis hatte immer einen negativen Kontext - die Gemeinschaft nahm ab, anstatt sich zu konsolidieren, was das Projekt irgendwie schwächer machte. Gleichzeitig, wie in der biologischen Evolution, sowie in ihrer von uns jetzt beobachteten techno-Version in Form der Wirtschaft, kommt es ständig zu Verzweigungen, die zur Entstehung neuer Lebensformen führen. Schauen Sie sich den Stammbaum des Lebens an. Jeder Ast darauf ist ein Fork, der in der Urzeit auftrat - eine Mutation brachte eine neue Art hervor, die mit ihren Stammesgenossen unvereinbar wurde. Ohne diesen Mechanismus wäre das Leben in seinem modernen Verständnis unmöglich. Die Fähigkeit von Organismen zu mutieren ermöglicht es ihnen, sich an sich ändernde Umweltbedingungen anzupassen.

Somit kommen wir zu dem Schluss, dass Konsens sowohl gut ist, weil er die Koordination vieler Akteure ermöglichtund Skalierbarkeit, aber gleichzeitig ist es schlecht, weil es das Entstehen von Neuem behindert, da diejenigen, die vom Konsens abweichen, immer als Außenseiter betrachtet werden und das System sie irgendwie "herausdrängt".

Im Gegensatz zur biologischen Evolution führen wir Menschen eine Techno-Evolution durch, und es hängt von uns ab, wie diese Evolution stattfindet. Die Quelle für Mutationen in der Technologie sind Ingenieure, die den wissenschaftlichen Ansatz und kreatives Denken anwenden, um neue, vielversprechendere und lebensfähigere Versionen von Technologie zu produzieren. Wir profitieren von häufigeren Mutationen, wir profitieren von Dezentralisierung im Bereich der Generierung neuer effektiver Lösungen sowohl in Bezug auf Technologie als auch Organisationen. Im Gegenteil - eine Situation, in der ein Standpunkt "gewinnt" und ein vollständiger und bedingungsloser Konsens erreicht wird, birgt das Risiko des Zusammenbruchs unter bestimmten Umständen. Deshalb ist die Natur nicht den Weg gegangen, Superorganismen in der Größe eines Planeten zu schaffen, sondern den Weg der Dezentralisierung gegangen - wenn alle Lebewesen über ökologische Nischen und Ökosysteme verstreut sind, was das Leben insgesamt stabil macht und sogar den Planeten Erde stabilisiert gemäß dem [Daisyworld-Modell](https://en.wikipedia.org/wiki/Daisyworld) (kurz gesagt, dank der Biosphäre und ihrer Vielfalt wird der Planet stabiler).

Warum also diese Abschweifung? In erster Linie möchte ich betonen, dass nicht alle Daten einen obligatorischen Konsens erfordern, wodurch die Blockchain für jedes Szenario ungeeignet ist. Konsens wird notwendig, wenn unkoordinierte Aktionen zu irreversiblen Konsequenzen führen könnten. Zum Beispiel ist es an Kreuzungen von Straßen entscheidend. Wenn es keine Einigung über Verkehrsregeln gibt, wann man fahren und wann man anhalten soll, kann man sich das potenzielle Chaos vorstellen!

Zurück zum Hauptthema des Artikels kann angenommen werden, dass Konsens dort angemessen ist, wo die Risiken hoch sind und umgekehrt in Fällen, in denen die Risiken gering sind, ist es angemessener, ein vollständig dezentrales System ohne Konsens zu wählen. Die Code-Entwicklung selbst ist eine relativ sichere Aktivität, solange sie im Zustand von Quell- oder sogar ausführbaren Dateien auf dem Build-Server bleibt. Wenn es jedoch darum geht, diese Dateien in die physische Infrastruktur der realen Welt zu implementieren, ist es sehr wichtig, konsistent zu sein. Deshalb haben wir für die ersten beiden Punkte unseres DevOps-Stacks (Entwicklung und Build) Projekte mit Gleichgesinnten ohne Konsens gewählt.

## Erste Phase - Entwicklung - Radicle

![](/blog/images/web3-devops-stack/1st-phase-development.png)

Also, bei der Code-Entwicklung sind vollständige Freiheit und Dezentralisierung erforderlich, um diegeneration der besten Lösungen direkt von den Autoren. Glücklicherweise ist in der Code-Entwicklungsbranche der de facto Standard zu einem dezentralisierten Versionskontrollsystem geworden. Ich spreche natürlich von git. git impliziert a priori nicht das Vorhandensein einer einzigen "Wahrheitsquelle" - jeder Benutzer muss das Repository auf seinen Computer klonen und mit seiner lokalen Kopie arbeiten, bevor er den Code verwendet. Darüber hinaus ist die Datenspeichermethode in git nichts anderes als eine Kette von Blöcken (feste Fakten von Codeänderungen - Commits), die die Unveränderlichkeit der Historie garantieren. Das heißt, git selbst ist eine Art Blockchain, deren Konsens durch die Autoren selbst manuell durch Branching und Merge-Anfragen erreicht wird.

Dennoch haben trotz der dezentralen Natur von git Web2-Plattformen ihren Platz eingenommen. Heutzutage ist die Code-Entwicklung fast ausschließlich um eine relativ kleine Anzahl von Plattformen wie Github, Bitbucket, Gitlab zentralisiert. Und das geschah genau wegen der Einführung zusätzlicher Tools: DevOps (CI/CD-Pipelines, integrierte Funktionen zur Erkennung von Schwachstellen in Abhängigkeiten des Quellcodes und vieles mehr) und Social Networking (Belohnungssysteme für Entwickler, Problemverfolgung, Projektmanagement). Diese Tools sind kein Bestandteil des ursprünglichen git-Protokolls und erschweren die Migration von Projekten von einer Plattform zur anderen.

Das Projekt [Radicle](https://radicle.xyz/) wurde speziell mit dem Ziel gegründet, Code-Entwickler von der Abhängigkeit von großen Plattformen zu befreien, die wir als die erste Komponente unseres Web3-DevOps-Stacks betrachten. Das Projekt hat eine recht [lange Geschichte](https://docs.radworks.org/community/our-story) und eine Reihe von bedeutenden Transformationen auf dem Weg zu seinem aktuellen Moment. Anfangs wurde Radicle auf Basis des damals an Popularität gewinnenden Inter Planetary File System (IPFS) aufgebaut, aber irgendwann erkannten die Entwickler, dass die Methoden zur Speicherung und Hashing von Code-Repository-Daten in IPFS mit den Methoden zur Speicherung in git unvereinbar waren, was zu einer Duplizierung von Informationen und einem übermäßigen Datenverkehr selbst bei kleinen Updates führte. Schrittweise wurde die Entscheidung getroffen, auf eine minimalistischere Lösung umzusteigen - den direkten Austausch von git-Patches, unter Verwendung des nativen [Pack-Protokolls](https://git-scm.com/docs/pack-protocol/en), um dies zum Hauptweg der Datenübertragung im "codekeeper"-Netzwerk zu machen. Diese Entscheidung markierte den Beginn eines umfangreichen Refactorings und Neuschreibens des Projekts von Go nach Rust. Die neue Version des Protokolls, genannt Heartwood,zieht Inspiration aus Projekten wie Secure Scuttlebutt (SSB) und dem [Lightning Network](https://en.wikipedia.org/wiki/Lightning_Network) von Bitcoin.

Später, im Jahr 2021, wurde die Dezentrale Autonome Organisation (oder einfach DAO) Radworks auf der Ethereum-Blockchain gegründet, der RAD-Governance-Token wurde ausgegeben und die notwendigen Mittel für die weitere Entwicklung des Projekts wurden aufgebracht. Offenbar leugnet das Projektteam nicht die Bedeutung sozialer und wirtschaftlicher Komponenten bei der Code-Entwicklung, versucht jedoch gleichzeitig nicht, alle unterstützenden Tools in ihre Implementierung zu integrieren. Eine solche Initiative ist ihr Projekt und der gleichnamige Smart Contract [Drips](https://www.drips.network/), der auf die automatische Verteilung von Spenden an Open-Source-Entwickler unter dem Motto "Finanzieren Sie Ihre Abhängigkeiten" abzielt. Innerhalb dieses Smart Contracts kann jedes Entwicklungsprojekt eine automatische Umverteilung der erhaltenen Spenden für sein Set von Abhängigkeiten (Pakete, Bibliotheken) einrichten.

Erst kürzlich, im März 2024, wurde die Version 1.0.0 der Implementierung des Heartwood-Protokolls veröffentlicht, was bedeutet, dass es bereits für Produktionszenarien von dezentralen Entwicklungs-Pipelines in Betracht gezogen werden kann.

## Zweite Phase - Build, Test, Kontinuierliche Integration - Fluence

![web3 build software](/blog/images/web3-devops-stack/2nd-phase-build.png)

Der nächste Schritt in unserer vereinfachten DevOps-Pipeline ist die Build-Phase, die mehr als nur die Code-Kompilierung umfasst. Es beinhaltet eine Reihe von Prozessen mit unterschiedlichen Rechenressourcen-Intensitäten. Diese Berechnungen führen jedoch nicht immer zu greifbaren Veränderungen. Mit anderen Worten führt nicht jede Code-Änderung oder Build-Initiierung zu einem Release. Oft laufen Continuous Integration (CI)-Pipelines nach einem Zeitplan. Die von ihnen produzierten Artefakte sind in der Regel kurzlebig und werden gelöscht, wenn sie nicht in einem Release enthalten sind. Daher betrachten wir diese Phase im Entwicklungslebenszyklus nicht als konsensbedürftig, ähnlich wie zahlreiche Cloud-Computing-Projekte, die Blockchain zur Ergebnisüberprüfung verwenden.

Die Entwickler von [Fluence](https://fluence.dev/) vertreten eine ähnliche Position. Dies ist ein Projekt, das Robonomics sehr nahe steht und ebenfalls intensiv libP2P als Transportebene und ipfs als Speicherebene in seinen Anwendungsfällen verwendet, während es sich auf die Orchestrierung von Peers und Berechnungen auf ihnen ohne die Notwendigkeit zentralisierter Plattformen konzentriert. Schauen wir sie uns genauer an.

Fluence besteht aus zwei Schlüsselkomponenten - Aqua und Marine. Ersteres ist eine domänenspezifische Sprache (DSL) und wird verwendet, um die Abfolge von Aufgaben zu verwalten.auf Rechengeräten, d.h. um Peers zu orchestrieren. Einerseits kann das Erlernen einer anderen Sprache viele Benutzer abschrecken, andererseits ist es ein ehrlicher Schritt, der Sie sofort auf die unvermeidliche Zukunft vorbereitet. Tatsache ist, dass die meisten CI-Plattformen in der Regel Konfigurationsdateien in gängigen Formaten wie YAML oder JSON für das Verwalten und Einrichten von Pipelines anbieten. Zunächst ist dies wirklich bequem und ermöglicht es jedem Benutzer ohne Programmierkenntnisse, mit der Arbeit zu beginnen, aber im Laufe der Zeit, wenn die Anforderungen und entsprechend die Anzahl der Konfigurationen wachsen, führt der Mangel an so vertrauten Werkzeugen für Programmierer zum Wachstum von Boilerplate-Code und der Unfähigkeit, die Komplexität zu verwalten. Versuche, YAML zu einer Konfigurationssprache mit Vorlagen zu machen, lösen das Problem ebenfalls nicht, was zur Entstehung von Ad-hoc-Konfigurationsbeschreibungssprachen wie HCL (HashiCorp Configuration Language) beiträgt. Aqua bietet eine sofortige Lösung in Form einer Anwendungssprache für den Fluss von Berechnungen, die auf einer zuverlässigen theoretischen Grundlage in Form von Pi-Kalkül beruht, in den Aqua-Code kompiliert wird, um ihn anschließend auf Peers auszuführen. Dies erhöht zwar die Einstiegshürde in die Technologie etwas, sollte aber idealerweise in Zukunft eine stabilere und wartbarere Arbeitsweise ermöglichen. Derzeit ist Aqua eine ziemlich niedrigstufige Sprache, aber im Laufe der Zeit können Bibliotheken entstehen, die bewährte Verfahren zur Gestaltung von Berechnungsflüssen unter Verwendung bequemer mathematischer Abstraktionen implementieren, was die Entwicklung verteilter Berechnungen beschleunigen wird.

Aqua legt die Reihenfolge der Berechnungen fest, aber die Berechnungen selbst werden mit Marine vorbereitet und durchgeführt - einem ebenfalls von Fluence entwickelten Komponenten. Marine ist ein SDK (Software Developers Kit) - eine Reihe von Tools zum Zusammenstellen von gegenseitig kompatiblen Webassembly-Modulen sowie eine Laufzeitumgebung - eine allgemeine Umgebung für deren Ausführung. Module sind relativ unabhängige Softwarekomponenten, von denen jede ihren eigenen Zustand speichert, aber über Funktionsimporte/-exporte miteinander interagieren kann. Eine Gruppe von interagierenden Modulen bildet einen Dienst, der komplexes Verhalten implementiert und als Akteur im Fluence-Peer-Netzwerk fungiert.

Zusammen bieten Aqua und Marine vollständig das, was als Denken eines cyber-physischen Systems bezeichnet werden kann - das heißt, die gesamte Vielfalt an Berechnungen, die dabei helfen, Entscheidungen darüber zu treffen, welche Maßnahmen in der Außenwelt ergriffen werden sollen, um die Chancen auf Erfolg im evolutionären Wettlauf zu erhöhen.

[Modellbasierte](https://en.wikipedia.org/wiki/Model-based_systems_engineering) Robotikentwicklung erfordert ressourcenintensive Berechnungen. Ein erheblicher Teil der Tests der entwickelten Software für Roboter wird in Simulatoren durchgeführt, und auf Verstärkungslernen basierende Algorithmen starten solche virtuellen Umgebungen hunderttausendfach.von Malen, bevor sie das gewünschte Verhalten des Agenten erreichen. Verschiedene Physik- und Rendering-Engines sowie darauf basierende Spiel-Engines können als virtuelle Umgebungen dienen. In letzter Zeit hat sich das Entitätskomponentensystem-Designmuster in diesen Umgebungen weit verbreitet. Übrigens verwendet die moderne Version des in engen Kreisen bekannten Robotersimulators Gazebo/ex-Ignition von Open Robotics (sie entwickeln auch ROS) auch [ECS](https://gazebosim.org/docs/harmonic/architecture), um die Leistung und Flexibilität zu steigern. Tatsächlich ist laut den Entwicklern von Fluence ihr Ausführungsmodell gut geeignet für die Implementierung von verteilten Architekturen, die auf diesem Prinzip aufbauen.

Natürlich muss kompilierte und getestete Software auf Hardware-Plattformen bereitgestellt werden, um einen Mehrwert zu bringen. Hypothetisch ist es auch möglich, Software mithilfe von Fluence bereitzustellen. Tatsächlich sagen die Entwickler selbst, dass ihr Stack auch die Bereitstellung auf Peers ermöglicht, und das ist tatsächlich der Fall. Wir sind jedoch der Meinung, dass Konsens in Bezug auf die Interaktion mit der externen Umgebung und physischer Ausrüstung erforderlich und wichtig ist.

## Dritte Phase - Bereitstellung - Robonomics

![Kunst der Robotikbereitstellung](/blog/images/web3-devops-stack/3rd-phase-deploy.png)

Ein wichtiges Merkmal von Software ist die Tatsache, dass sie die Außenwelt nicht von selbst verändert, sondern als Modell der umgebenden Realität dient und dabei hilft, darüber nachzudenken - Hypothesen aufzustellen, einen Aktionsplan vorzuschlagen, Prozesse in der Außenwelt mithilfe von Ausrüstung oder Personen zu starten. Ein Problemverfolgungssystem ist beispielsweise selbst kein System, das die Außenwelt verändert, sondern kann als digitaler Zwilling des Entwicklungsteams bezeichnet werden, der seinerseits Veränderungen ins Leben bringt. Dank des Problemverfolgungssystems kann das Team Aktionen miteinander koordinieren, sich gegenseitig helfen. Mit anderen Worten ermöglicht das Problemverfolgungssystem die Aufrechterhaltung eines Konsenses innerhalb des Entwicklungsteams - wer welche Aufgaben erledigt und wann - aber die Änderung selbst wird von Menschen vorgenommen.

Bei der Diskussion über weltverändernde Systeme beziehen wir uns auf physische Entitäten, die in der realen Welt wie intelligente Geräte, Roboter und autonome Fabriken operieren. Im Wesentlichen dient Robonomics als Brücke von Informationssystemen und Menschen zur externen Welt über Robotersysteme. Die Bedeutung von Sicherheit kann nicht genug betont werden, da deren Vernachlässigung nicht nur zu Kapitalverlusten, sondern auch zu realen Katastrophen führen könnte.

Trotz der kritischen Natur dieser Systeme erschienen die ersten sicheren dezentralen Speichersysteme nicht im Industriesektor, sondern im Finanzsektor. Bankkonten erwiesen sich alsEin attraktiveres Ziel für Computerkriminelle als Haushaltsgeräte, von denen die meisten keine Netzwerkverbindung hatten. Die Technologie, die zur Sicherung von Bankinformationssystemen verwendet wird, hat sich erheblich weiterentwickelt, von SWIFT bis Ethereum, und wird nun allmählich in anderen Wirtschaftssektoren implementiert.

Robonomics steht an vorderster Front dieser sicheren, dezentralen Netzwerke und übernimmt die Verantwortung für die letzte Phase des Web3 DevOps-Förderbands - die Bereitstellung.

Im Allgemeinen bedeutet ein Deployment, ein System in Betrieb zu nehmen. Ein vertrauterer Synonym für diesen Begriff ist Installation (install) - ein Ereignis, nach dem das produzierte und gelieferte System (eine Instanz eines bestimmten Gerätetyps, eine ausführbare Datei von Software) innerhalb eines übergeordneten Systems zu funktionieren beginnt. Was ist in diesem Fall gut an der Blockchain? Vor allem ist es immer wichtig für uns zu verstehen, was genau veröffentlicht wurde, wenn wir eine beliebige Version des Programms in die Außenwelt entlassen. Während der Entwicklung können sich die Versionen sehr oft ändern, und das ist normal, daher ist es nicht ratsam, alle Informationen darüber in die Blockchain zu setzen. Das Ereignis der Veröffentlichung oder des Starts muss jedoch festgehalten werden, damit wir bei der Entgegennahme von Feedback klar verstehen können, mit welcher genauen Version von Quellcodes oder Zeichnungen wir dieses Feedback vergleichen müssen. Die Blockchain kann als zuverlässiges Repository für Informationen über alle Softwareupdates von daran angeschlossenen Geräten dienen.

Darüber hinaus muss der Installationsprozess konsistent sein, da Abweichungen zu Konfigurationskonflikten führen können. Diese Konflikte können durch nicht übereinstimmende Softwaremodulversionen, das Bereitstellen unterschiedlicher Systeme im selben Raum und andere damit zusammenhängende Probleme entstehen. Zum Beispiel ist die zuvor erwähnte Autokollision an einer unregulierten Kreuzung ein Beispiel für einen solchen Zusammenstoß. Im Kontext der Softwarebereitstellung können Konflikte als API-Verletzungen, unzureichende Hardwarefähigkeiten oder das Schreiben an verbotene oder unsichere Speicherorte auftreten. Robonomics kann die Konfigurationen der angeschlossenen Geräte speichern und auf Algorithmusebene des Konsenses solche Zustände verhindern.

Sie können einen speziellen [Start](https://wiki.robonomics.network/docs/launch/) verwenden, um Software im Robonomics-Parachain bereitzustellen, mit dem Sie einen Knoten mit zusätzlichen Parametern an die Blockchain anschließen können. Der Parameter kann ein eindeutiger Identifikator im IPFS-Content-Distribution-Netzwerk sein, von dem aus Sie ein Software-Image, eine Binärdatei, den Quellcode für die Konfiguration des Betriebssystems oder sogar ein Bash-Skript erhalten können! Da jede Transaktion im Netzwerk mit einem kryptografischen Schlüssel signiert ist, ist ein solcher Aufruf im Parachain im Wesentlichen mit einer öffentlichen Freigabesignatur gleichzusetzen.

Für komplexere Szenarien können Sie einen [digitalen Zwilling](https://wiki.robonomics.network/docs/digital-twins/), das es ermöglicht, eine Korrespondenztabelle zwischen beliebigen Daten von 256 Bits Länge und einem Konto im Robonomics-Netzwerk festzulegen. Auf diese Weise können Sie eine Protokolldatei für Konfigurationsversionenänderungen für Gerätekonten im Robonomics-Netzwerk führen. In herkömmlichen Konfigurations- und Bereitstellungssystemen werden Hosts in der Regel als Knoten angegeben - dies sind Computer mit einem DNS-Namen oder einer IP-Adresse. Im Falle von Web3 werden Hosts dank ihrer öffentlichen kryptografischen Schlüssel identifiziert, an die Konten angehängt sind. Um die Konfiguration zu ändern, können Sie einen neuen Inhaltsidentifikator hinzufügen, von dem das Gerät eine neue Version der Software empfangen und aktualisieren wird.

## Alles zusammenfügen

![web3 devops stack](/blog/images/web3-devops-stack/devop-stack-full-art.png)

Also, lassen Sie uns das aus der Vogelperspektive betrachten. Menschen bilden den Kern unseres großen kyberphysischen Systems. Menschen sind chaotisch, unvorhersehbar... und das ist gut! Sie generieren neue Bedeutungen, Ideen, Produkte. Sie aktualisieren ihren Willen, die Welt zu verändern. Heutzutage ist es nicht mehr notwendig, die Welt mit den eigenen Händen zu verändern. Es gibt Aufgaben vor der Menschheit, für die menschliche Hände nicht das geeignetste Werkzeug sind. Stattdessen gibt es Maschinen, die jetzt als Vermittler unseres Willens dienen. Maschinen hingegen sind streng deterministisch und vorhersehbar. Und die Menschen lieben das, ja. Sie lieben es, wenn der Zug pünktlich am Bahnhof ankommt und die Qualität der Produkte, die sie konsumieren, immer vorhersehbar ausgezeichnet ist. Dafür nutzen die Menschen Netzwerke. Viele Netzwerke! Sie generieren Ideen im Radicle-Netzwerk, Maschinen sammeln und testen sie zu ausführbaren Modulen im Fluence-Netzwerk und setzen sie dann auf Robotern im Robonomics-Netzwerk ein. Die Roboter wiederum verändern die Umgebung, und ihre Sensoren, auch über Robonomics, geben den Menschen Feedback, um eine Entscheidung zu treffen - der Zyklus ist geschlossen. Dies ist ein Zyklus kontinuierlicher Verbesserungen, in dem jeder seinen Platz hat. Es gibt keinen Widerspruch zwischen Maschine und Mensch - beide schaffen in Harmonie eine neue Ordnung der Menschheit - eine interplanetare Menschheit.